# knowledge-distillation

#### Definition

Knowledge distilliation is a technique to transfer the knowledge of a trained neural network to a smaller neural network with minimum loss of information.

![image](https://user-images.githubusercontent.com/77073029/183123422-f4602ced-edc7-4fb0-a627-59d3a4d8085a.png)
