# knowledge-distillation

Knowledge distilliation is a technique to transfer the knowledge of a trained neural network to a smaller neural network with minimum loss of information.
